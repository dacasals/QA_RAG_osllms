{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "662b09650ece43e4b8b8df586769f580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_027a4d4980334afca6cd968c4dec66e1",
              "IPY_MODEL_a9041217aaf84709903229408f817092",
              "IPY_MODEL_717ce79eed6641a593b770b8559da679"
            ],
            "layout": "IPY_MODEL_815e9bb1216d477b8cee95ce689f4e43"
          }
        },
        "027a4d4980334afca6cd968c4dec66e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa926360ff148ab8a9d2017343d30e8",
            "placeholder": "​",
            "style": "IPY_MODEL_868adb0d8c644e978ad3849f50777ad2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a9041217aaf84709903229408f817092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b32d5df542ad422cbdca0e99c467a235",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb0425882c144c0482fb06622fd1ccde",
            "value": 3
          }
        },
        "717ce79eed6641a593b770b8559da679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19abe0dd49a9449d84f79c429cd35410",
            "placeholder": "​",
            "style": "IPY_MODEL_af8b2d8ebc674751a16756959cd9b493",
            "value": " 3/3 [01:12&lt;00:00, 23.95s/it]"
          }
        },
        "815e9bb1216d477b8cee95ce689f4e43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa926360ff148ab8a9d2017343d30e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868adb0d8c644e978ad3849f50777ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b32d5df542ad422cbdca0e99c467a235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0425882c144c0482fb06622fd1ccde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19abe0dd49a9449d84f79c429cd35410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af8b2d8ebc674751a16756959cd9b493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch datasets transformers langchain playwright html2text sentence_transformers faiss-cpu"
      ],
      "metadata": {
        "id": "ZcO4yeYS-WeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6QeyZ8bY9g6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !playwright install\n",
        "# !playwright install-deps"
      ],
      "metadata": {
        "id": "rRFQLm4q9hTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes trl\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "2xBXaICq_eO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What include this notebook\n",
        "- Load a Mistral 7B model with quantization config.\n",
        "- Compare base model answers vs a simple RAG version.\n",
        "- As documents  for RAG use the dataset created (text from Web and Pdfs) using public data extracted from https://www.thoughtworks.com, check the other notebook to be see the scrapper. https://drive.google.com/file/d/1GzSC4F0uGoHGcRHl9FEdvA3WLA7NIjvp/view?usp=drive_link\n"
      ],
      "metadata": {
        "id": "q65XN6bgittY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel\n",
        "\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import AsyncChromiumLoader\n",
        "\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "wE2zyhRv_u5o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################\n",
        "# Tokenizer\n",
        "#################################################################\n",
        "\n",
        "model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          trust_remote_code=True,\n",
        "                                          device_map='auto',\n",
        "                                          padding_side=\"left\",\n",
        "                                          add_eos_token=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "#################################################################\n",
        "# bitsandbytes parameters\n",
        "#################################################################\n",
        "\n",
        "# Activate 4-bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False\n",
        "\n",
        "#################################################################\n",
        "# Set up quantization config\n",
        "#################################################################\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "# Check GPU compatibility with bfloat16\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "#################################################################\n",
        "# Load pre-trained config\n",
        "#################################################################\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "662b09650ece43e4b8b8df586769f580",
            "027a4d4980334afca6cd968c4dec66e1",
            "a9041217aaf84709903229408f817092",
            "717ce79eed6641a593b770b8559da679",
            "815e9bb1216d477b8cee95ce689f4e43",
            "cfa926360ff148ab8a9d2017343d30e8",
            "868adb0d8c644e978ad3849f50777ad2",
            "b32d5df542ad422cbdca0e99c467a235",
            "eb0425882c144c0482fb06622fd1ccde",
            "19abe0dd49a9449d84f79c429cd35410",
            "af8b2d8ebc674751a16756959cd9b493"
          ]
        },
        "id": "61bpy7iX_yD_",
        "outputId": "b14c5c68-d898-4534-b51a-ed53f05597d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "662b09650ece43e4b8b8df586769f580"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = model.num_parameters()\n",
        "print(f\"num params:\", total_params)\n",
        "trainable_params = model.num_parameters(only_trainable=True)\n",
        "print(f\"num trainable params:\", trainable_params)\n",
        "print(f\"PCT trainable\", (trainable_params/total_params) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwDCkrRh_Mkn",
        "outputId": "71d17d8d-f766-40f6-9e30-38091bd6f83c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num params: 7241732096\n",
            "num trainable params: 262410240\n",
            "PCT trainable 3.6235839233122604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "            print(name, \"trainable\")\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n",
        "print(print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "LlRwyXyIBDTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "w-85cEveBjHr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = model.num_parameters()\n",
        "print(f\"num params:\", total_params)\n",
        "trainable_params = model.num_parameters(only_trainable=True)\n",
        "print(f\"num trainable params:\", trainable_params)\n",
        "print(f\"PCT trainable\", (trainable_params/total_params) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW7RhpZXDABC",
        "outputId": "f8dbc5bd-4248-4778-c0f0-0b6d30d8e609"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num params: 7241732096\n",
            "num trainable params: 0\n",
            "PCT trainable 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    #temperature=0.2,\n",
        "    device_map='auto',\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=800,\n",
        "    batch_size=16 # This does not work\n",
        ")"
      ],
      "metadata": {
        "id": "8J0vMWFeD0SG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline )"
      ],
      "metadata": {
        "id": "O6tofUfvD4w9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVogw4T-D84s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
        "\n",
        "Here is context to help:\n",
        "{context}\n",
        "\n",
        "### QUESTION:\n",
        "{question} [/INST]\n",
        " \"\"\"\n"
      ],
      "metadata": {
        "id": "o7nON_iHERYe"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create prompt from prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        "\n",
        ")\n",
        "\n",
        "# Create llm chain\n",
        "llm_chain = LLMChain(llm=mistral_llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "Pqla38UQEcZ8"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Call to the base Mistral model without context"
      ],
      "metadata": {
        "id": "L-vmieGbiAH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm_chain.invoke({\"context\": \"\", \"question\": \"What is the buisness model of Thoughtworks?\"})\n",
        "print(resp[\"text\"].split(\"[/INST]\")[1].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlh_Gpp-FvKi",
        "outputId": "0af6e0c4-571e-4096-8211-5d25f5857ee7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thoughtworks is a global technology consulting firm that follows a unique business model called \"agile software development\" or \"agile consulting.\" They provide services primarily in software development, digital platform engineering, and strategic consulting. Their clients range from startups to large enterprises across various industries.\n",
            "\n",
            "Thoughtworks' revenue comes mainly from project-based engagements where they collaborate with their clients to design, build, and operate software solutions using agile methodologies. They do not sell any proprietary software or hardware but instead focus on delivering customized solutions tailored to each client's needs.\n",
            "\n",
            "References:\n",
            "1. About Us - Thoughtworks: https://www.thoughtworks.com/about\n",
            "2. Services - Thoughtworks: https://www.thoughtworks.com/services\n",
            "3. Business Model - Thoughtworks (not explicitly mentioned but can be inferred from their mission statement and services offered)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(\"./thoughtworks_cleaned_dataset.parquet\")"
      ],
      "metadata": {
        "id": "hw0I3wxSJWc1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def templated_text(row):\n",
        "  \"\"\"\n",
        "  Prepare templated text to be embedded using FAISS and HF model\n",
        "  \"\"\"\n",
        "  template = f\"\"\"\n",
        "  Source: {row[\"url\"]}\n",
        "  Title: {row[\"title\"]}\n",
        "  {'Document language: ' + row[\"lang\"] if \"lang\" in row else \"\"}\n",
        "  {row[\"text\"]}\n",
        "  \"\"\"\n",
        "  return template\n",
        "df[\"final_text\"] = df.apply(templated_text, axis=1)"
      ],
      "metadata": {
        "id": "W6CJ2vyqJqrb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in df.sample(2)[\"final_text\"].values:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzeaMuqGpsju",
        "outputId": "574af7b3-ace4-489b-8193-c475679563fc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Source: https://www.thoughtworks.com/en-in/about-us/partnerships/cloud/microsoft\n",
            "  Title: Partnerships: Microsoft\n",
            "  Document language: en\n",
            "  Microsoft and Thoughtworks are partnering to help our clients to leverage cloud along every step of their transformation journey.\n",
            "\n",
            "Benefiting from the strong platform, tools and resources provided by Microsoft, alongside our global delivery expertise, we help our clients to strengthen their core technology foundation, build seamless data-driven customer experiences and unlock new revenue steam opportunities for their evolving business.\n",
            "  \n",
            "\n",
            "  Source: https://www.thoughtworks.com/en-ca/insights/topic/security\n",
            "  Title: Security\n",
            "  Document language: en\n",
            "  Security is so much more than just table stakes for today’s digital business: it goes to the heart of trust in the relationship you build with your customers.\n",
            "\n",
            "High profile breaches and increased public awareness of security and privacy issues have resulted in a loss of trust. We need to rebuild. At the same time, the scale and sophistication of threats grow by the day.\n",
            "\n",
            "The only way to stay ahead of the curve is through the implementation of multidisciplinary security practices that combine continuous delivery with a focus on privacy and security in depth.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# Chunk text\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
        "\n",
        "docs = text_splitter.create_documents(texts=df[\"final_text\"].values.tolist())\n",
        "chunked_documents = text_splitter.split_documents(docs)\n",
        "\n",
        "# Load chunked documents into the FAISS index\n",
        "import sys\n",
        "import os\n",
        "emb_model=HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')\n",
        "db = FAISS.from_documents(\n",
        "    chunked_documents,\n",
        "    emb_model)\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "kBbzolWeI1-F"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3uodLVGStJM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.save_local(\"./dbthoughtworks_faiss.db\")"
      ],
      "metadata": {
        "id": "GEfPXFZ6vYhl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Call the model with the in-memory FAISS retriever"
      ],
      "metadata": {
        "id": "hyogF_p8hl8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jGnTcPbuhiuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What is your recommendation platform to create machine learning model projects?\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER8r9FTrPqVO",
        "outputId": "20c1e356-99f1-4be6-d7cb-1d2a4cbcec4d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide references links if available. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content='Title: Guide to evaluating MLOps\\n  Source: https://www.thoughtworks.com/what-we-do/data-and-ai/cd4ml/guide-to-evaluating-mlops-platforms\\n  Document language: \\n  Find the right platform to accelerate your AI journey.\\n\\nThere’s a plethora of tools and platforms to help organizations get machine learning models into production. However, the amount of options can be overwhelming and navigating the trade-offs is difficult. Should we buy or build a platform? When buying, which choices should we consider? What should be the key selection criteria? Just understanding which software to evaluate can be confusing.'), Document(page_content='According to a VentureBeat report from 2019, 87% of data science projects never make it into production. MLOps software can play a crucial part in making sure the investments in ML don’t go to waste and the models end up where they should: in production, producing value. The Guide to Evaluating MLOps Platforms helps you navigate the space, highlights the trade-offs and shows how to perform an evaluation to choose the best solution for your organization, boost your machine learning initiatives and drive forward your AI-powered products.'), Document(page_content=\"Title: gradio-app/gradio: Build and share delightful machine learning apps, all in Python. 🌟 Star to support our work!\\n  Source: https://github.com/gradio-app/gradio\\n  Document language: \\n  Gradio is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then share a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. No JavaScript, CSS, or web hosting experience needed!\\n\\nIt just takes a few lines of Python to create a beautiful demo like the one above, so let's get started 💫\\n\\nWe recommend installing Gradio using , which is included by default in Python. Run this in your terminal or command prompt:\"), Document(page_content='What programming language should such a team use?\\n\\n Remember the rise of JavaScript: using JavaScript and NodeJS empowered the same people to work on both the frontend and the backend of a system (\"fullstack\").\\n\\nPython, being a general purpose programming language, offers the same convenience. You can use its scientific stack for ML development and leverage its frameworks (Django, Flask, FastAPI) for model deployment, providing predictions behind a REST or gRPC API.\\n\\n Sweet, isn\\'t it?\\n• You want your ML algorithm or ML framework to be adopted: you write it in Python (or you provide Python bindings for it using FFI);\\n• The Python ecosystem just got bigger.\\n\\nWe will probably still use Python tomorrow to write ML software.')]\n",
            "\n",
            "### QUESTION:\n",
            "What is your recommendation platform to create machine learning model projects? [/INST]\n",
            "  Based on the provided context, Thoughtworks recommends considering MLOps platforms to help organizations get machine learning models into production. They suggest using the \"Guide to Evaluating MLOps Platforms\" document to navigate the space, understand trade-offs, and perform an evaluation to choose the best solution for your organization. This document is available at <https://www.thoughtworks.com/what-we-do/data-and-ai/cd4ml/guide-to-evaluating-mlops-platforms>.\n",
            "\n",
            "As for creating machine learning model projects directly, the context mentions Gradio as an open-source Python package that allows you to quickly build a demo or web application for your machine learning model, API, or any arbitrary Python function. It's recommended to install Gradio using pip, which is included by default in Python. More information about Gradio can be found at <https://github.com/gradio-app/gradio>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "for row in df[\"questions\"].values:\n",
        "  questions.extend(row)\n",
        "questions = list(set(questions))\n",
        "len(questions)"
      ],
      "metadata": {
        "id": "_DH2AtwuT8ls"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "results2 = []\n",
        "index = 0\n",
        "for q in questions:\n",
        "  print(index)\n",
        "  res = rag_chain.invoke(q)\n",
        "  results2.append((res[\"question\"], res[\"context\"], res[\"text\"].split(\"[/INST]\")[1]))\n",
        "  index += 1"
      ],
      "metadata": {
        "id": "ZJFQ78yGaVQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "with open('results2.pickle', 'wb') as handle:\n",
        "    pickle.dump(results2, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "FO0ZsfWPNGgC"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print some questions extracted with regex explicitly from text\n",
        "for result in results2[:10]:\n",
        "  print(\"Question: \", result[0])\n",
        "  print(\"Context: \", result[1])\n",
        "  print(\"Answer: \", result[2])\n",
        "  print(\"_____________________________________________\")"
      ],
      "metadata": {
        "id": "kk6AJbSXBMfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bdb83b1-6fef-4dd2-9549-59f361d543a5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  At what scale does it pay off to fine-tune a model with your organization’s code?\n",
            "Context:  [Document(page_content='specific use cases, e.g. by combining a reusable architecture and tech stack definition with user stories to generate task plans or test code, similar to what my colleague Xu Hao is describing here. Prompt composition applications like this are most commonly used with OpenAI’s models today, as they are most easily available and relatively powerful. Experiments are moving more and more towards open source models and the big hyperscalers hosted models though, as people are looking for more control over their data. As a next step forward, beyond advanced prompt composition, people are putting lots of hopes for future improvements into the model component. Do larger models, or smaller but more specifically trained models work better for coding assistance? Will models with larger context windows enable us to feed them with more code to reason about the quality and architecture of larger parts of our codebases? At what scale does it pay off to fine-tune a model with your organization’s code? What will happen in the space of open source models? Questions for a future memo. Thanks to Kiran Prakash for his input This is a little story of generating a median function, and how it illustrates the usefulness and limitations of LLM-assisted coding. I needed to calculate the median of a list of numbers in a Typescript codebase. Instead of what I would usually do, which is start an internet search for “median function javascript”, I tried to get GitHub Copilot to assist me. // calculate the'), Document(page_content=\"Source: https://www.thoughtworks.com/en-th/radar/techniques/rush-to-fine-tune-llms\\n  Title: Rush to fine-tune LLMs | Technology Radar\\n  Document language: en\\n  As organizations are looking for ways to make large language models (LLMs) work in the context of their product, domain or organizational knowledge, we're seeing a rush to fine-tune LLMs. While fine-tuning an LLM can be a powerful tool to gain more task-specificity for a use case, in many cases it’s not needed. One of the most common cases of a misguided rush to fine-tuning is about making an LLM-backed application aware of specific knowledge and facts or an organization's codebases. In the vast majority of these cases, using a form of retrieval-augmented generation (RAG) offers a better solution and a better cost-benefit ratio. Fine-tuning requires considerable computational resources and expertise and introduces even more challenges around sensitive and proprietary data than RAG. There is also a risk of underfitting, when you don't have enough data available for fine-tuning, or, less frequently, overfitting, when you have too much data and are therefore not hitting the right balance of task specificity that you need. Look closely at these trade-offs and consider the alternatives before you rush to fine-tune an LLM for your use case.\"), Document(page_content=\"Source: https://www.thoughtworks.com/en-in/radar/techniques/rush-to-fine-tune-llms\\n  Title: Rush to fine-tune LLMs | Technology Radar\\n  Document language: en\\n  As organizations are looking for ways to make large language models (LLMs) work in the context of their product, domain or organizational knowledge, we're seeing a rush to fine-tune LLMs. While fine-tuning an LLM can be a powerful tool to gain more task-specificity for a use case, in many cases it’s not needed. One of the most common cases of a misguided rush to fine-tuning is about making an LLM-backed application aware of specific knowledge and facts or an organization's codebases. In the vast majority of these cases, using a form of retrieval-augmented generation (RAG) offers a better solution and a better cost-benefit ratio. Fine-tuning requires considerable computational resources and expertise and introduces even more challenges around sensitive and proprietary data than RAG. There is also a risk of underfitting, when you don't have enough data available for fine-tuning, or, less frequently, overfitting, when you have too much data and are therefore not hitting the right balance of task specificity that you need. Look closely at these trade-offs and consider the alternatives before you rush to fine-tune an LLM for your use case.\"), Document(page_content=\"Source: https://www.thoughtworks.com/en-es/radar/techniques/rush-to-fine-tune-llms\\n  Title: Rush to fine-tune LLMs | Technology Radar\\n  Document language: en\\n  As organizations are looking for ways to make large language models (LLMs) work in the context of their product, domain or organizational knowledge, we're seeing a rush to fine-tune LLMs. While fine-tuning an LLM can be a powerful tool to gain more task-specificity for a use case, in many cases it’s not needed. One of the most common cases of a misguided rush to fine-tuning is about making an LLM-backed application aware of specific knowledge and facts or an organization's codebases. In the vast majority of these cases, using a form of retrieval-augmented generation (RAG) offers a better solution and a better cost-benefit ratio. Fine-tuning requires considerable computational resources and expertise and introduces even more challenges around sensitive and proprietary data than RAG. There is also a risk of underfitting, when you don't have enough data available for fine-tuning, or, less frequently, overfitting, when you have too much data and are therefore not hitting the right balance of task specificity that you need. Look closely at these trade-offs and consider the alternatives before you rush to fine-tune an LLM for your use case.\")]\n",
            "Answer:  \n",
            "  According to the documents provided by Thoughtworks, fine-tuning a model with an organization's code may not always be necessary or cost-effective. The documents suggest that using Retrieval-Augmented Generation (RAG) instead offers a better solution in the vast majority of cases. Fine-tuning requires considerable computational resources and expertise, and introduces additional challenges around sensitive and proprietary data. Additionally, there is a risk of underfitting or overfitting when fine-tuning. Therefore, it is recommended to carefully consider the trade-offs and alternatives before deciding to fine-tune a model with an organization's code.\n",
            "\n",
            "  Source:\n",
            "  - <https://www.thoughtworks.com/en-th/radar/techniques/rush-to-fine-tune-llms>\n",
            "  - <https://www.thoughtworks.com/en-in/radar/techniques/rush-to-fine-tune-llms>\n",
            "  - <https://www.thoughtworks.com/en-es/radar/techniques/rush-to-fine-tune-llms>\n",
            "_____________________________________________\n",
            "Question:  If you can take that first step in product thinking by looking at what the customer wants, what does your target customer actually need?\n",
            "Context:  [Document(page_content='Customer-centricity is the foundation on which product thinking is built. Being genuinely geared toward customer needs means moving beyond data-derived conclusions, to base decisions on real-world testing and interactions that put you directly in the customer’s shoes - even when they’re not a comfortable fit.\\n\\n“There are companies where every executive has to work one day a month in the call center. Right there on the frontline with other employees, taking calls, they develop empathy for the end-user and gain a better understanding of their customer’s needs, or the problems they face. It gives executives a firsthand, human-to-human view that can be very insightful.”\\n\\nToo often, products are solutions in search of problems, or involve the use of technology for its own sake. Development informed by product thinking starts with identifying the specific issue or need the product will resolve and building accordingly, leveraging the right mediums to deliver in a way that exceeds expectations and creates long-lasting connections.\\n\\nExcelling at customer experience can require enterprises to challenge - and change - their own structures and cultural norms. By raising tolerance for failure, tasking development to tightly focused, highly autonomous teams and providing a technology platform that can be drawn from again and again to support an entire product portfolio, organizations can innovate at speed and scale.'), Document(page_content=\"Emma: Yes, that's the one, yeah! It's not just about looking at current problems that customers have. You then also need to be looking at the future as well. So bringing in other ideas because customers don't always know what they want. With the iPod, did customers say that they wanted an iPod? No. So sometimes, it's just about, also, then finding a gap in the market and creating something for that, as well; and you can't always bring that through necessarily from existing data. I guess what you can do, when you're speaking to customers, is you can see gaps, and then from that, you can see opportunities, and from that, you can create solutions to solve those, which can generate new ideas and new products. So you need to bring those different kind of lenses in and bring that through the organization.\\n\\nThe biggest mistake I've seen is when executives have an idea, and they say we want this. And you ask them why, and they say “because our customers are going to love it.” OK, fine… Let's test it with customers. And you test it with the customers. And they say, no way. I do not want that. And all the data just proves that they really would hate to have that added.\"), Document(page_content=\"What we did was we built a mobile product that streamlines the whole sales process. It equips the sales consultants with all the tools to help the customer choose their dream car without leaving their site. They get access to all the vital information that they need, such as product specs, customization, and pricing, looking for perhaps their dream configuration of the car across different stores, and applying for financial services, all the important parts of getting a sale done, but without leaving the customer.\\n\\nIt saved them millions of dollars in licensing fees for the existing tools that weren't doing the job very well. The business got better data from increased adoption. Most importantly, the customers had a more memorable and engaging experience.\\n\\n[00:17:27] Kimberly: I'll say, too, right, the automotive industry typically probably not thought in the forefront of leaders of digital products. I think this proves that when you're working this way, adopting this mindset, maybe even more traditional industries can see a lot of great digital product success working this way. Love both of those examples. We talked quite a bit about what product thinking is. I'd love to maybe take the flip side and say, what are some of the common misconceptions when it comes to product thinking, or what is not product thinking?\"), Document(page_content='Source: https://www.thoughtworks.com/perspectives/edition6-product-innovation\\n  Title: Product Innovation| Perspectives\\n  Document language: \\n  For the modern enterprise, creating a quality digital experience is crucial to successfully delivering products and services, maintaining client relationships and, ultimately, securing revenue streams and competitive advantage. Yet research shows effective experience is something firms often struggle to deliver. The solution is a new approach to design and execution that revolves around customer needs and emphasizes constant improvement. It’s a state of mind called product thinking - and these are its principles.\\n\\n\\ufeffThe ‘project mindset,’ where a team builds software or a product to order, hands it off, then immediately moves on to the next thing, is no longer fit for purpose. In product thinking, teams take ownership over the product throughout its entire lifecycle, factoring in feedback and evolving the product to deliver an even better experience over time.\\n\\n“Product thinking recognizes that digital experiences deliver value to customers in the same way that physical products and services do, so they need to be continually enhanced and refreshed to stay competitive.”')]\n",
            "Answer:  \n",
            "  According to the provided context, the first step in product thinking is to identify the specific issue or need the product will resolve for the customer. However, it's also mentioned that sometimes, it's necessary to find a gap in the market and create something for that, even if customers don't explicitly express their need for it. Therefore, while looking at what the customer wants is important, it's also essential to consider what they truly need. This can be discovered through direct interactions with customers, observing their behaviors and pain points, and staying attuned to emerging trends and technologies. (Source: Thoughtworks Perspectives - Product Innovation)\n",
            "\n",
            "Reference: https://www.thoughtworks.com/perspectives/edition6-product-innovation\n",
            "_____________________________________________\n",
            "Question:  Rebecca: How do you decide in that fourth pattern, what's the right granularity of checks to be making?\n",
            "Context:  [Document(page_content='Rebecca: How do you decide in that fourth pattern, what\\'s the right granularity of checks to be making? In many architectural decisions, is this too big or too small? It\\'s the classic kind of problem. What\\'s your sense on how you characterize what is the right size of one of these checks, these individual steps in that pattern?\\n\\nCarl: I think like anything, it depends. It strikes me that that question is very similar to a question like, \"How big should my microservice be?\" Well, it should be as big, but no bigger than it needs to be. There is no one size fits all. The point to keep in mind is that you can start with what you think is the best granularity but like any other product, if you treat compliance like a product, what you\\'re really trying to do is enable the teams to deliver software safely. That means that the granularity of what you\\'re trying to deliver, it\\'s based on how your teams are using it.\\n\\nYou don\\'t have to find the perfect solution right out of the gate. You can make a guess. Try it, deliver it, see how people are using it. You\\'ll get feedback to say, \"Oh, I would like that, but I only want 70% of that thing that you delivered to me. I want to refactor it and I want the freedom to change part of it.\" How you deliver it is really based on how your customers are going to use it. Your customers are the other development teams within your organization.'), Document(page_content=\"Rebecca: One of the things I was wondering about a feature, if you will, of patterns is that you can talk about when they are applicable. Given the fact that some of the things that you're talking about are somewhat fundamental notions, there are, for example, lots of ways you might decide to use distributed time. As you were looking at these different code bases, did you mostly run across essentially the same implementations, or were there times when there might have been different implementations for one of these more fundamental constructs that you were able to examine?\\n\\nUnmesh: Yes. I think again, the strengths of the patterns approach is, you look at multiple things, and the rule of three is generally followed that you look at at least three things before you call something as a pattern and then there is this concept of similarity. In various systems the implementations are similar, but not exactly the same. Patterns allow you to document what similar thing is across these systems. There can be variations that you then document as well.\"), Document(page_content=\"I think, say, the third thing that's important about patterns after the breaking the large topic into chunks and this focus on recurring solutions, is that we come up with a system of names for these solutions. Now, sometimes these names are out there in the world. When we talk about ordering of these different transactions, we often talk about these things called Lamport clocks, and that's a name that's out there in the world. A lot of people know about Lamport clocks already. Some of the names are fairly well known, but maybe not completely the case like high watermark, low watermark, leases are fairly well known.\\n\\nSome might be particular to-- Well, we've got to coin them ourselves. When we talk about-- Unmesh is currently working on a pattern called version vector, which is somewhat used but not completely widely used. The point is, within the system of patterns that we're building, we construct a set of names who are consistent within this pattern family. Obviously, you want to try and use names that are well known out there, but sometimes, these names get a bit confused, so you have to pick on something and give it a greater degree of rigour than you might otherwise get.\"), Document(page_content='Kief: I think our kind of lens on it is given, say, a new project, or working with a new client, these are the things that we would expect the team to start with the saying, like, \"We assume we should try to work in these ways and use these practices because we think they\\'re generally effective,\" to start from there. It might be that there are reasons for a particular situation where a particular practice might not be the right thing to do, but we start with the assumption that is probably right. Let\\'s then discuss if we think this is an exception, why we think it\\'s an exception.\\n\\nNeal: Well, that gets to your distinction about a pattern because one of the important things about patterns is they work within a context, and it comes attached with the context. It is not just a universal thing that always applies.\\n\\nBirgitta: It\\'s a really good-- Mike, your question, what then does it mean when we write these down? What does it mean for people on teams? Just recently, there was also a discussion on chat again, or I often hear from people that they say, \"Oh, it feels like this is mandated. This is like a mandate. We have to do this, right?\" Or it\\'s being-- to use some stronger language, \"has been shoved down our throats,\" right? That\\'s actually exactly not the intention.')]\n",
            "Answer:  \n",
            "  According to Carl in the provided context, the decision on the right granularity of checks in the fourth pattern depends on how the development teams within the organization are using it. He suggests starting with a guess, delivering it, and getting feedback to refactor it based on the teams' needs. Therefore, there isn't a definitive answer on the right granularity of checks, and it varies depending on the specific context and usage by the development teams. (Source: Document(page_content='Carl: I think like anything, it depends. You can make a guess. Try it, deliver it, see how people are using it. You’ll get feedback to say, \"Oh, I would like that, but I only want 70% of that thing that you delivered to me. I want to refactor it and I want the freedom to change part of it.\" How you deliver it is really based on how your customers are going to use it. Your customers are the other development teams within your organization.')\n",
            "_____________________________________________\n",
            "Question:  Which then helped us understand in marketing language, what does this mean for you?\n",
            "Context:  [Document(page_content='Source: https://www.thoughtworks.com/es-ec/what-we-do/customer-experience-product-design\\n  Title: Experiencia del Cliente, Producto y Diseño\\n  Document language: es\\n  Sorprende a tus clientes, supera a tus competidores y marca el futuro de tu experiencia de cliente (CX).\\n\\nConviértete en un líder de la experiencia del cliente (CX)\\n\\nLas organizaciones obsesionadas con el cliente dominan sus mercados ofreciendo productos y experiencias extraordinarios.\\n\\nConvertirse en un líder de CX no es fácil, y las transformaciones de CX pueden crear más preguntas que respuestas. Preguntas como:\\n• None ¿Cómo podemos priorizar las oportunidades y responder rápidamente a las cambiantes expectativas de los clientes?\\n• None ¿Cómo podemos utilizar nuestros datos para añadir continuamente valor al cliente?\\n\\nAprovechando la experiencia de un socio experto, puede crear y evolucionar productos y experiencias potentes que aporten un valor sostenido al cliente.\\n• None Acelera el tiempo de obtención de valor: prueba y aprende rápido, impulsa la adopción y apoya la innovación continua que ofrece un potente impacto empresarial.\\n• None Potencia tu organización: crea experiencias únicas en tus propios términos y fomenta una mentalidad centrada en el cliente y en los resultados.\\n• None Maximiza el ROI de CX: invierte en los productos y experiencias adecuados para mitigar el riesgo, impulsa el compromiso del cliente y protege y aumenta los ingresos.'), Document(page_content='Source: https://www.thoughtworks.com/es-cl/what-we-do/customer-experience-product-design\\n  Title: Experiencia del Cliente, Producto y Diseño\\n  Document language: es\\n  Sorprende a tus clientes, supera a tus competidores y marca el futuro de tu experiencia de cliente (CX).\\n\\nConviértete en un líder de la experiencia del cliente (CX)\\n\\nLas organizaciones obsesionadas con el cliente dominan sus mercados ofreciendo productos y experiencias extraordinarios.\\n\\nConvertirse en un líder de CX no es fácil, y las transformaciones de CX pueden crear más preguntas que respuestas. Preguntas como:\\n• None ¿Cómo podemos priorizar las oportunidades y responder rápidamente a las cambiantes expectativas de los clientes?\\n• None ¿Cómo podemos utilizar nuestros datos para añadir continuamente valor al cliente?\\n\\nAprovechando la experiencia de un socio experto, puede crear y evolucionar productos y experiencias potentes que aporten un valor sostenido al cliente.\\n• None Acelera el tiempo de obtención de valor: prueba y aprende rápido, impulsa la adopción y apoya la innovación continua que ofrece un potente impacto empresarial.\\n• None Potencia tu organización: crea experiencias únicas en tus propios términos y fomenta una mentalidad centrada en el cliente y en los resultados.\\n• None Maximiza el ROI de CX: invierte en los productos y experiencias adecuados para mitigar el riesgo, impulsa el compromiso del cliente y protege y aumenta los ingresos.'), Document(page_content='Source: https://www.thoughtworks.com/es-es/what-we-do/customer-experience-product-design\\n  Title: Experiencia del Cliente, Producto y Diseño\\n  Document language: es\\n  Sorprende a tus clientes, supera a tus competidores y marca el futuro de tu experiencia de cliente (CX).\\n\\nConviértete en un líder de la experiencia del cliente (CX)\\n\\nLas organizaciones obsesionadas con el cliente dominan sus mercados ofreciendo productos y experiencias extraordinarios.\\n\\nConvertirse en un líder de CX no es fácil, y las transformaciones de CX pueden crear más preguntas que respuestas. Preguntas como:\\n• None ¿Cómo podemos priorizar las oportunidades y responder rápidamente a las cambiantes expectativas de los clientes?\\n• None ¿Cómo podemos utilizar nuestros datos para añadir continuamente valor al cliente?\\n\\nAprovechando la experiencia de un socio experto, puede crear y evolucionar productos y experiencias potentes que aporten un valor sostenido al cliente.\\n• None Acelera el tiempo de obtención de valor: prueba y aprende rápido, impulsa la adopción y apoya la innovación continua que ofrece un potente impacto empresarial.\\n• None Potencia tu organización: crea experiencias únicas en tus propios términos y fomenta una mentalidad centrada en el cliente y en los resultados.\\n• None Maximiza el ROI de CX: invierte en los productos y experiencias adecuados para mitigar el riesgo, impulsa el compromiso del cliente y protege y aumenta los ingresos.'), Document(page_content=\"So an example there, I was working with a telecommunications company and they have a team around their mobile phones division, and they have a team around their home internet division, and they have a team around their cable services division. And what we found when we did some initial research is, what the consumer wants is a solution for their digital lifestyle.\\n\\nAnd so you look at opportunities like parental controls, for instance. Parents are really concerned about how much time their kids are spending on screens and the content they're consuming, but they have to manage a separate set of parental controls for every channel the kid's on. And so a company that was really concerned about solving a digital lifestyle problem would be, what if we gave you one place to put all your parental controls in it, propagated across all these products, right?\\n\\nSo it's just an example of, are you solving the customer's problem or are you a feature factory that is just I've got these products that I'm rolling out these features, and every quarter I have to put new features on in order to get my bonus, and am I really solving the customer's problem?\")]\n",
            "Answer:  \n",
            "  Based on the provided context, the text emphasizes the importance of becoming a leader in Customer Experience (CX) by offering extraordinary products and experiences to clients. It also mentions that prioritizing opportunities and responding quickly to changing client expectations, utilizing data to add value, accelerating the time to value, empowering the organization, and maximizing the ROI of CX are key aspects of achieving this goal. The example given illustrates the concept of solving a customer's digital lifestyle problem instead of being a feature factory.\n",
            "\n",
            "In marketing language, this means focusing on creating exceptional customer experiences that differentiate your brand from competitors and meet the evolving needs of your audience. By understanding and addressing their pain points, anticipating their expectations, and delivering personalized solutions, you can build strong relationships, increase loyalty, and drive growth. Additionally, leveraging data insights and continuous innovation will enable you to stay ahead of the competition and adapt to market trends.\n",
            "_____________________________________________\n",
            "Question:  So thinking of it from the leader's perspective, what are some of the things or key signs that leaders can be on the look out for to try to recognize when maybe some of these threats or challenges to collaboration are kind of creeping into their teams?\n",
            "Context:  [Document(page_content=\"Kimberly Boyd: Good ones, right, and I think ones that make a lot of sense. And I think especially important as enterprises are trying to scale, right? You were talking initially about how there's teams and then their go-to and everyone goes to them for everything and then they're in that prioritization cycle. But you need to bring other people in the boat with them, otherwise they're never going to have that multiplier that collaboration can bring.\\n\\nSo thinking of it from the leader's perspective, what are some of the things or key signs that leaders can be on the look out for to try to recognize when maybe some of these threats or challenges to collaboration are kind of creeping into their teams?\"), Document(page_content='Lisa Kwan: Yeah, good question. Picking up signs is always one of the things we want to do sooner rather than later. When I think about that question, some immediate ones include you\\'re going to see frustration, you\\'re going to see resentment. Especially, let\\'s go back to the low hanging fruit, when time and priorities aren\\'t clear, aren\\'t matched up and trade offs are being forced to make. Of course your groups, your team members are going to be frustrated. They\\'re probably logging in a lot of extra time, probably not even getting acknowledgement that they\\'re doing it because this is their job, do it. And then of course that can escalate into resentment. And also not having shared understanding between groups can, like I mentioned, people may start thinking, \"Oh, that group\\'s a terrible collaborator. They\\'re never reliable,\" et cetera. And that can lead to more and more rooted ideas about one another that are not helpful for future collaborations either.\\n\\nBut all in all, I would say some signals I see in the companies that I work with are what I generally call counter collaborative behaviors. So groups will stall. They will do things to stall the initiative or they\\'ll do things like data dumping, which is quite an interesting one because data dumping sometimes comes in the form of, \"Look, we\\'re collaborating. We\\'re going to give all this data to the other group.\"'), Document(page_content='Lisa Kwan: I would say that it is very specific to the root cause of those behaviors. So for example, one area that I\\'ve actually done a little bit of writing about is the fact that lots of times collaboration between groups can be a threat to groups because they\\'re actually asked to share this information that has kept them differentiated and important in the company or to give up their autonomy in certain ways. The critical thing is there are many reasons why a group might stall an initiative. It could be because they feel that it\\'s taking away the value that they bring to an organization or it could just be, \"Look, this is not something we do. We don\\'t want to get involved with this. This is not us. We don\\'t want to do this.\"\\n\\nSo to be able to diagnose, you go to what is the root cause first, then you can take steps to address it. I don\\'t think I actually answered your question in terms of someone turning around that stalling, but essentially you can say ... Oh, well, let\\'s say a group says, \"This is not what we do.\" I have seen companies where savvy leaders have been like, \"Okay, we recognize this. How is this related to what the group does?\" And so they reframe it like, \"Oh, you\\'re the innovation team and now you have to go talk to customers. Well, you\\'re really going out there to get new ideas to innovate some more while serving a different purpose.\" But you can start addressing things if you know what the root cause is.'), Document(page_content='Kimberly: Reframing is an incredibly powerful tool when used properly. Just so we don\\'t leave people feeling that it\\'s a hopeless endeavor to try to tackle and succeed when it comes to collaboration, what in your experience are the ingredients for successful collaboration? Where can leaders really start to cultivate these?\\n\\nLisa Kwan: I can answer this question in two ways. One is in terms of the system\\'s perspective, as in what\\'s actually happening in the collaboration as a system or in the organization as a system, and from the leader perspective, which is what can the leader do. The first, in terms of the system\\'s perspective, I would say that the conditions that I discussed at the Thoughtworks 2019 Paradigm Shift conference are where I would start. So if leaders have an awareness or a framework of where to look and they prioritize the time to diagnose, that\\'s a huge step.\\n\\nNow, one of the problems with collaboration is that it\\'s an inherently unwieldy phenomenon. So sometimes, I don\\'t know if you have this, but friends or an acquaintance might say to you, regarding let\\'s say a colleague or even a spouse, \"I just don\\'t understand him or she\\'s so complicated.\" Well, you think one person\\'s complex, put them in groups and then put those groups together and that gives you-')]\n",
            "Answer:  \n",
            "  According to Lisa Kwan in the provided context, some key signs that leaders can look out for to recognize threats or challenges to collaboration in their teams include:\n",
            "\n",
            "1. Frustration and resentment among team members due to unclear priorities, time pressures, and forced trade-offs.\n",
            "2. Lack of shared understanding between groups leading to negative perceptions and entrenched ideas about one another.\n",
            "3. Counter-collaborative behaviors such as stalling initiatives or data dumping.\n",
            "\n",
            "The root causes of these behaviors vary, and leaders should diagnose the underlying issues before attempting to address them. For instance, a group might stall an initiative because they feel it takes away their value or autonomy within the organization. By recognizing these signs early on, leaders can take steps to foster a more collaborative environment and prevent potential conflicts from escalating.\n",
            "\n",
            "Source: The context provided in the given documents. (Thoughtworks 2019 Paradigm Shift conference talks by Kimberly Boyd and Lisa Kwan)\n",
            "_____________________________________________\n",
            "Question:  Is there anything that worth for folks to look at?\n",
            "Context:  [Document(page_content='Others see value in more explicitly financial terms,  \\nsuch as ROI or improving attractiveness to investors.\\n+\\n+\\n+'), Document(page_content='It sounds like there\\'s a lot of really great content that leads to rich conversation that does address some of these things that you\\'ve now mentioned, which I think is really important and great. However, I do wonder, if someone were to ask: how is Looking Glass different from all of the other trend reports and surveys and things that are out there? What would be your response to that, Ken?\\n\\nSo, really, the biggest difference is it\\'s not just observation. It\\'s really easy to do. There\\'s tools where you can say: what has people searched for the most on Google? And then you can write a trend report over this as the hottest technology. And that\\'s interesting, certainly. And I\\'ll be honest, we use that as some of our inputs. What are people looking at? But then we actually research it and try to give active advice.\\n\\nSo the lenses breakdown into these trends that we put in a grid. And some of them, we put a time horizon on, as Rebecca said. We\\'re seeing this now, this is happening today in the market; or we\\'re just starting to, or it\\'s on the horizon. But again, that\\'s factual information. But then we also say, based on the rest of what\\'s going on in the world, we\\'re going to say, \"Even though all of these are happening right now, these are the ones you should adopt. These are the ones you really should look at. These are the ones. Analyze them, take a look at it and see if it meets ... because it might fit your particular market if it doesn\\'t fit somebody else\\'s, et cetera.\"'), Document(page_content=\"So the value is obvious and it's obvious quickly, right? I mean, what I love about your message, Gary, is for the listeners out there who are leaders in digital transformations. I mean, the story of Unqork and what you're doing, it is one of hope and optimism because you're like, there are new and different ways of kind of wrestling this bear to the ground, right. There's this thing that is the demand from the business for digital. And you in particular, because you used to be inside the machine, you know their pain, you know the minds of the CIOs, good, bad and indifferent.\\n\\nAnd I genuinely think that most resistance comes from sort of noble places, right? And that can be fear, right? But there's a nobility to think, hey, there's a future here and I'm not sure if I'm a part of it and that scares me a little bit. But as you rightly say, the visionary ones are the ones that can see the end from the beginning, right? And they know that in that future, there is room for everybody. But the future is coming very soon and we're all going to have to live in it so we might as well soon have embrace it.\"), Document(page_content='an increasingly valuable roadmap for enterprises keen to extend and make optimal use of their \\ndata resources in a consistently ethical way.  \\n•\\t Decentralized personal data marketplaces that give consumers more sovereignty over their \\npersonal data are contributing to the development of a personal information economy. This trend \\nwill have significant implications for the way companies store, analyze and use information about \\ntheir customers, and the subsequent development of enterprise ecosystems.   \\nThings to consider (Analyze)\\nThings to watch for (Anticipate)\\nRealizing value from data and AI platforms')]\n",
            "Answer:  \n",
            "  Based on the provided context from Thoughtworks documents, the company provides trend reports and insights with a focus on providing active advice rather than just observing and reporting facts. They use various sources including Google search trends but go beyond that by researching and analyzing the trends to provide recommendations. The trends cover various areas such as technology, data usage, and enterprise ecosystems. Some specific trends mentioned include decentralized personal data marketplaces and the development of a personal information economy.\n",
            "\n",
            "Therefore, for folks looking to extend and make optimal use of their data resources in a consistently ethical way, or those interested in the latest technology trends and their implications for businesses, the reports and insights provided by Thoughtworks could be worth looking at.\n",
            "\n",
            "Sources:\n",
            "- Document(page_content='It's really easy to do. There's tools where you can say: what has people searched for the most on Google? And then you can write a trend report over this as the hottest technology. And that's interesting, certainly. And I'll be honest, we use that as some of our inputs. What are people looking at?' - This suggests that Thoughtworks uses various sources including Google search trends as inputs for their trend reports.\n",
            "- Document(page_content='So the lenses breakdown into these trends that we put in a grid. And some of them, we put a time horizon on, as Rebecca said. We\\'re seeing this now, this is happening today in the market; or we\\'re just starting to, or it\\'s on the horizon.' - This indicates that Thoughtworks provides recommendations on which trends to adopt based on the current market situation and the future implications.\n",
            "- Document(page_content='an increasingly valuable roadmap for enterprises keen to extend and make optimal use of their data resources in a consistently ethical way.  •\tDecentralized personal data marketplaces that give consumers more sovereignty over their personal data are contributing to the development of a personal information economy. This trend will have significant implications for the way companies store, analyze and use information about their customers, and the subsequent development of enterprise ecosystems.' - This specifically mentions the trend of decentralized personal data marketplaces and its implications for businesses.\n",
            "_____________________________________________\n",
            "Question:  What are taboo features?\n",
            "Context:  [Document(page_content='Source: https://www.thoughtworks.com/insights/blog/diversity-equity-and-inclusion/what-privileges-do-you-have\\n  Title: What privileges do you have?\\n  Document language: \\n  For sure, representation may not be present in popular culture and there is not much that a common person can do about it. The thing is that I’m pretty sure there were lesbians on my village when I was growing up, only that people did not call them lesbians, but they called them \"friends\" who live together. It looked like they were not openly out and this is the next topic I wanted to discuss.\\n\\nSo, as you can imagine, even when I realised liking other women was a possibility, I still had to come out before I could be proud and open on the Lesbian Visibility Day or Pride Month. It is just one more barrier that, even when having a safe space, it can be stressful for some people. In my case, it made me feel shy about opening up to so many people and anxious about what would be the response.'), Document(page_content='unintentionally excluding people due to temporary, situational or permanent disability. When \\nthe ICT accessibility team is creating products, they want to avoid designing products that \\nmake people feel excluded and create products that are accessible to people – regardless \\nof their gender, age, ability or location.\\nSo it isn’t enough to want to be inclusive and accessible: designing for inclusion starts with \\nrecognizing exclusion. \\nGo to Action on Accessibility >\\n“If you do not intentionally, deliberately and \\nproactively include, you will unintentionally exclude” \\nJoe Gerstandt Author, speaker\\nAwareness: Accessibility'), Document(page_content='• You must not use sensitive data or categories that are otherwise prohibited under applicable laws in creating your remarketing lists. Sensitive data may include, but is not limited to, political affiliation, religion, sexual orientation, gender identity, sexually transmitted diseases, mental health conditions such as anxiety or depression, terminal illness, and other diseases.\\n• Remarketing is not allowed for disallowed content, products, or services.\\n• Underage users. You must not create a remarketing list, retarget or otherwise profile any individuals under the age of 18, or other applicable age restrictions in the market(s) where you operate. Additionally, you must not target individuals under the minimum age required for the product advertised. For example, you may not target alcohol advertisements to individuals under the applicable minimum drinking age.\\n• Web properties directed to children. Audience remarketing must not be implemented on sites or applications directed to children under the age of 18, or other applicable age limitations in the market(s) where you operate.\\n• Tracking tag misuse. Modifying, changing or otherwise tampering with the UET tracking tag and tag script is strictly forbidden, other than for uses expressly authorized by Microsoft. Learn more about UET implementation and usage.\\n• Precise geo-location on mobile devices. If you elect to pass to Microsoft the precise location of an end user, you must do so only with the end user’s express consent.'), Document(page_content='Source: https://www.thoughtworks.com/en-ec/insights/blog/diversity-equity-and-inclusion/what-privileges-do-you-have\\n  Title: What privileges do you have?\\n  Document language: en\\n  For sure, representation may not be present in popular culture and there is not much that a common person can do about it. The thing is that I’m pretty sure there were lesbians on my village when I was growing up, only that people did not call them lesbians, but they called them \"friends\" who live together. It looked like they were not openly out and this is the next topic I wanted to discuss.\\n\\nSo, as you can imagine, even when I realised liking other women was a possibility, I still had to come out before I could be proud and open on the Lesbian Visibility Day or Pride Month. It is just one more barrier that, even when having a safe space, it can be stressful for some people. In my case, it made me feel shy about opening up to so many people and anxious about what would be the response.')]\n",
            "Answer:  \n",
            "  Based on the provided context, there is no mention of \"taboo features\" in the Thoughtworks documents. Therefore, the answer is 'NOT FOUND'.\n",
            "_____________________________________________\n",
            "Question:  We had a true feedback culture where anyone could give me feedback as a CEO anytime, and there were days where I'd have someone else say, \"Can I give you some feedback?\n",
            "Context:  [Document(page_content='We\\'re really wanting to be leaders who, even if you hear feedback that doesn\\'t quite gel with your experience of yourself, it\\'s still so important to stay present and to hear it, to thank them, and to perhaps then go and test it with someone else, but never, ever argue about it because that\\'s going to make sure that you\\'ll never get feedback again. I think feedback, while for many it causes you to want to run away as soon as you hear someone say, \"Can I give you some feedback?\" it can trigger a lot in ourselves. It\\'s really, really important to make sure that you\\'re able to stay present.\\n\\nI know I was CEO of a group of psychologists for many years. We had a true feedback culture where anyone could give me feedback as a CEO anytime, and there were days where I\\'d have someone else say, \"Can I give you some feedback? Internally I\\'m thinking, \"Oh my god, not again,\" but it really taught me the value of hearing it, of valuing it, not necessarily acting on it, but never dismissing it and never being arrogant enough to think that my view of myself was worth any more than what this person had had the courage to come and tell me.\\n\\nI think that is that kind of humility that we talk about that you need as leader, but then with your head, what are you going to do about it? Is it something you actually need to do differently? Do you need to go and get some other perspectives?'), Document(page_content='I think having the ability to ask for feedback is a critically important skill that every modern leader has. It sounds trite to say that, but just think about the leaders you know who never ask you for feedback. They\\'re probably a leader who isn\\'t someone who\\'s focused on the impact they\\'re having. They\\'re not particularly so forward [crosstalk]\\n\\n[00:16:16] Kimberly: They\\'re not scoring high on the self-awareness.\\n\\n[00:16:18] Kirstin: No, they\\'re not. They\\'re frustrating to deal with because I know I\\'ve worked with leaders where I think, \"Oh, if I could just someone,\" whether it\\'s me or someone else tell you that when we\\'re in meetings, \"Could you just stop talking over the top of everyone?\" Perhaps they don\\'t even realize it. I know I\\'ve been in situations where I\\'ve then had the courage to speak up and give feedback, and either the person is totally unprepared to accept it or argues or whatever, and so you never try again. Don\\'t be that leader.'), Document(page_content='Results from our research highlighted that feedback was both the biggest obstacle to growth and the biggest engine for growth. From all Thoughtworkers that were interviewed, 50% said “I receive constructive feedback that supports my growth,” and around 30% said, “I don’t receive enough quality feedback at the right time.”\\n\\n\\n\\n Uncovering the problem: how might we strengthen our feedback culture?\\n\\nTo deepen our understanding, we ran quality interviews with 40 people. We learned that what helps people grow at Thoughtworks is:\\n• None Cultivation (grow yourself while growing others)\\n• None An environment of learning by doing\\n\\nWhen it comes to feedback, we realized that Thoughtworkers were not receiving enough feedback at the right time. They also tended to avoid difficult conversations, which\\n\\nInterviews with leadership reinforced the need to empower Thoughtworkers to have timely feedback conversations as well as difficult conversations. They said, “the way we currently deal with feedback can hurt our business.”\\n\\nWhen someone has performance issues in the team, and nobody addresses it, it gets worse, and the People Team (commonly known as HR) has to intervene. Timely feedback and difficult conversations could have helped to avoid these unpleasant escalations'), Document(page_content='Results from our research highlighted that feedback was both the biggest obstacle to growth and the biggest engine for growth. From all Thoughtworkers that were interviewed, 50% said “I receive constructive feedback that supports my growth,” and around 30% said, “I don’t receive enough quality feedback at the right time.”\\n\\n\\n\\n Uncovering the problem: how might we strengthen our feedback culture?\\n\\nTo deepen our understanding, we ran quality interviews with 40 people. We learned that what helps people grow at Thoughtworks is:\\n• None Cultivation (grow yourself while growing others)\\n• None An environment of learning by doing\\n\\nWhen it comes to feedback, we realized that Thoughtworkers were not receiving enough feedback at the right time. They also tended to avoid difficult conversations, which\\n\\nInterviews with leadership reinforced the need to empower Thoughtworkers to have timely feedback conversations as well as difficult conversations. They said, “the way we currently deal with feedback can hurt our business.”\\n\\nWhen someone has performance issues in the team, and nobody addresses it, it gets worse, and the People Team (commonly known as HR) has to intervene. Timely feedback and difficult conversations could have helped to avoid these unpleasant escalations')]\n",
            "Answer:  \n",
            "  According to the provided context, the speaker mentions having a true feedback culture as a CEO where anyone could give her feedback at any time. This suggests that the company values open communication and encourages giving and receiving feedback regularly. The documents also mention that feedback is both the biggest obstacle to growth and the biggest engine for growth at Thoughtworks. However, there is no specific information given about any policies or practices related to feedback within the company beyond the anecdote about the speaker's experience as a CEO. Therefore, there is no clear answer to the question about the specific process or guidelines for giving and receiving feedback within Thoughtworks, and no source link can be provided.\n",
            "\n",
            "However, the documents suggest that the company recognizes the importance of feedback and encourages open communication, and that there is a need to address performance issues in a timely manner through feedback conversations. Additionally, the documents mention that Thoughtworkers tend to avoid difficult conversations, and that leadership has recognized the need to empower Thoughtworkers to have timely feedback conversations as well as difficult conversations to avoid unpleasant escalations.\n",
            "\n",
            "Therefore, it can be inferred that the company values open communication and encourages regular feedback, but there may not be a formalized process or policy in place beyond encouraging individuals to have timely and difficult feedback conversations as needed.\n",
            "_____________________________________________\n",
            "Question:  Is it because you think these types of leaders have learned how to unlearn, or is there a bit of luck in there, or is it the business model or is it the product?\n",
            "Context:  [Document(page_content=\"I think what I find with leaders who have that sort of aha moment of that insight that they start to recognize the way they change companies is role modeling the behaviors they want to see other exhibit. That starts to have this sort of network effect. People start copying what leaders do. So if leaders are constantly experimenting, trying new things, sharing what's worked or not worked for them, everyone in your company is going to start doing that.\\n\\nSam: Do you think that's the secret to success for when you see or when we see companies that are doing extremely well or they're the buzz company of the moment? Do you think it's because of the inherent leadership style that they've adapted? Is it because you think these types of leaders have learned how to unlearn, or is there a bit of luck in there, or is it the business model or is it the product? Do you think that the success of those businesses is actually on the shoulders of the leader?\"), Document(page_content=\"But those leaders are probably using the same behaviors they used when they were startup. When there's only three people everyone knew exactly what they're building. They're not communicating to people like what our vision and mission that we're trying to achieve. They value output in terms of success. And really what they need to do is they start need to unlearn themselves because at the start maybe shipping stuff was great, but now they've got people that are new to the company and won't know how the company operates. So they need to start thinking about what's the actual outcomes and they need to not measure their success in terms of how much stuff they've got done. It's how much stuff they've allowed other people to get done.\\n\\nSo these are like very subtle shifts in behavior that need to happen as a leader when you're sort of scaling up companies from startup to scale up to these enterprise organizations. And a lot of the behaviors that made you successful in one moment or one paradigm can actually limit your success as you move to new paradigms, new situations and new circumstances.\\n\\nIt's one of these reasons why you've got to get really good at identifying what's the outcomes you're trying to drive and are the behaviors you're using driving those outcomes? Because if they're not, it's not the other person's fault. It's not that team that needs to change. It's you that needs to change, you need to own that.\"), Document(page_content=\"Barry O'Reilly: I think when you're in a big company, we hold up these leaders as inspiration for us. What are their behaviors that they're doing that are making them successful as a leader, right? Everybody wants to codify everything. Everybody wants to small steps, sort of three things that I can do to be Jeff Bezos. Do you think there's only three things Jeff Bezos does to be who he is? But what these leaders really cultivate and what I'm noticing the more I work with them is they're great experimenters. They're constantly looking for opportunities to get outside their comfort zone. I sort of call it, they're comfortable with being uncomfortable. They're actively creating scenarios where they's sort of at the edge of their excellence. They're sort of pushing themselves to do something a little bit more tougher, a little bit new for them.\\n\\nBut the way they manage that is they're really good at designing sort of safe to fail experiments. They think big, but they start small and learn fast what works and what doesn't, so they can iterate very quickly.\\n\\nSo just like a product has features and you need to constantly innovate the features of your product for it to stay relevant in the market. I think great leaders are really good at iterating their behaviors to adapt to the market that they're in, trying things and seeing what works and what doesn't.\"), Document(page_content=\"Because as a leader, you're up there on your perch and you have your own perspective, but that might be very different to what say an employee who's on the frontline with customers is actually seeing and observing right now. So when you can get a team of people to use and leverage their diverse perspectives and their self and situational awareness, you can actually turn that into a strategic awareness and a strategic advantage. And in studying the teams that I've been studying over the last seven months, these are elite teams that are trained to thrive in conditions of ambiguity, pressure, uncertainty, change. They say, I'm including looking at teams like say military special forces, or we've also had the privilege of studying the New Zealand All Blacks, which are their rugby team.\")]\n",
            "Answer:  \n",
            "  According to the provided documents, the speakers suggest that successful leaders have learned how to unlearn and adapt their behaviors as their companies grow. However, they also acknowledge that there may be elements of luck involved, and that business models and products can contribute to a company's success. The documents do not provide definitive answers on the relative importance of each factor.\n",
            "\n",
            "Source:\n",
            "- Document 1: Page content discussing the importance of role modeling and experimentation for leaders.\n",
            "- Document 2: Page content discussing the need for leaders to shift their behaviors as their companies grow and the importance of identifying desired outcomes.\n",
            "- Document 3: Page content discussing the importance of great experimenters and iterative behaviors in effective leadership.\n",
            "\n",
            "Link to the original Thoughtworks article where these documents were extracted from: https://insights.thoughtworks.com/blog/great-leaders-are-great-experimenters/\n",
            "\n",
            "Please note that this interpretation is based on the information available in the provided documents and may not reflect the complete thoughts or perspectives of Thoughtworks or its employees.\n",
            "_____________________________________________\n",
            "Question:  How to create an Operator?\n",
            "Context:  [Document(page_content=\"If you can't do that, like you cannot do in Go or Java or in some other languages, then writing expressions becomes really painful, if you can't use the infix operators. Operator overloading is an example of a taboo feature that turns out that, okay, people probably did misuse it in the '80s or '90s, with C++, but people learned.\\n\\nI see Python and Ruby are both languages that are more than 25 years old, and I don't see a huge problem in those languages overusing operator overloading. I actually see them benefiting the community much more than becoming a problem. I'm going on about this thing of taboo features, because another taboo feature is syntactic macros, which is something that is really important in this world in Lisp and Scheme, those languages have this idea.\\n\\nIt's much more powerful than macros in C, which is a simple string substitution. Macros in this pack executable, and they change the abstract syntax tree. They're safer and much more powerful. Actually, a lot of the reserved words of Lisp and Scheme are implemented as macros. In Lisp, this also had a bad story, because this feature was added early on in the language, and then Lisp grew at a time when free software was not a thing, Open Source was not a thing.\"), Document(page_content='The following YAML manifest illustrates the attributes specified within the Operator YAML:\\n\\nYou can also pass the Azure-specific Secrets seperately.\\n\\nThe table below summarizes the attributes of the Operator-specific section:\\n\\nFor in-depth information about the attributes of the Operator-specific section, please consult the Attributes of Operator-specific section.\\n\\nAfter creating the YAML configuration file for the Operator Resource, it\\'s time to apply it to instantiate the Resource-instance in the DataOS environment. To apply the Operator YAML file, utilize the command.\\n\\nTo ensure that your Operator has been successfully created, you can verify it in two ways:\\n\\nCheck the name of the newly created Operator in the list of Operators where you are named as the owner:\\n\\nAlternatively, retrieve the list of all Operators created in your organization:\\n\\nYou can also access the details of any created Operator through the DataOS GUI in the Operations App.\\n\\nNow once we have created an Operator, we would need to create a Resource (external Resource) which is an Azure Data Factory pipeline that would be managed by this Operator. In DataOS, the process of creating external Resources involves utilizing a specific type of Resource called, aptly, \"Resource.\" These Resources are categorized as second-class Resources, distinct from the first-class Resources like Workflow, Service, and Policy.'), Document(page_content=\"If you need to delete an operator, you first need to delete all the workloads or Resources that are dependent on it. Once it's done, use the command to remove the specific operator from the DataOS instance:\\n\\nHow does the Operator Work?¶\"), Document(page_content='The Operator alleviates collaboration hurdles arising from the diversity of technologies, each with its own frameworks, protocols, and languages. It achieves this by enabling cross-functional collaboration through a unified interface.\\n\\nThe Operator enhances the developer experience and enhances productivity by offering a unified and consistent interface across all external resources. While it does not simplify the underlying architecture, it significantly streamlines interaction with the infrastructure.\\n\\nHow to create an Operator?¶\\n\\nLet’s take a scenario in which a data developer wants to trigger a pipeline run in Azure Data Factory (external platform) and synchronize the metadata from ADF back into DataOS for observability. But before proceeding ahead, make sure you have the following prerequisites.\\n\\nTo set up your Azure Data Factory Operator, ensure you have the following four fields readily available:\\n\\nFor programmatic triggering, gather information regarding the various fields required for initiating a pipeline run. Refer to the official Azure Data Factory Pipeline Run API documentation for details.\\n\\nCreating your operator involves a series of logical steps, as outlined below:\\n\\nThe first step in creating an Operator is implementing three key interfaces: Reconciler, OController, and Message Handler Interface using the the DataOS SDK-Go.\\n\\nThe Reconciler Interface is responsible for reconciling resources based on state changes.')]\n",
            "Answer:  \n",
            "  To create an Operator in DataOS, follow these general steps as described in the given context:\n",
            "\n",
            "1. Ensure you have the necessary prerequisites, including having the required fields for programmatic triggering, such as information for initiating a pipeline run in Azure Data Factory. Refer to the official Azure Data Factory Pipeline Run API documentation for details.\n",
            "\n",
            "2. Implement three key interfaces: Reconciler, OController, and Message Handler Interface using the DataOS SDK-Go.\n",
            "\n",
            "3. The Reconciler Interface is responsible for reconciling resources based on state changes.\n",
            "\n",
            "4. The OController Interface handles incoming requests and returns responses.\n",
            "\n",
            "5. The Message Handler Interface processes messages sent to the Operator.\n",
            "\n",
            "6. Register the Operator with DataOS.\n",
            "\n",
            "7. Apply the Operator YAML file to instantiate the Resource-instance in the DataOS environment.\n",
            "\n",
            "For detailed instructions and implementation, refer to the official DataOS documentation or contact Thoughtworks for further assistance.\n",
            "\n",
            "Source Links:\n",
            "- [DataOS Operator Documentation](https://docs.dataos.io/)\n",
            "- [Azure Data Factory Pipeline Run API Documentation](https://docs.microsoft.com/en-us/rest/api/datafactory/)\n",
            "_____________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OtMLc3YiDRvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Calling the Chain with The retrievers\n",
        "\n",
        "quest = \"What Thoughworks thinks Terragrunt is valuable for?\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)"
      ],
      "metadata": {
        "id": "eDlvTG8tS-GC"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEzFrkT3TIjz",
        "outputId": "9628149c-1ae7-40d1-fdb2-071ec5b2dbab"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide references links if available. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content=\"Source: https://www.thoughtworks.com/en-th/clients/terrascope\\n  Title: Terrascope\\n  Document language: en\\n  Climate change has made mitigating carbon emissions increasingly critical for individuals, governments and businesses globally.\\n\\nBusiness operations and their supply chains account for the majority of emissions. Despite this, almost 85% of companies are unable to comprehensively measure their indirect emissions, especially those originating from sources they don't control or own (known in the industry as ‘Scope 3’ emissions). This makes accurately reporting and mitigating their carbon footprints near impossible.\\n\\nBacked by Olam Ventures, Terrascope is a Singapore-based climate-tech venture founded in 2021 to empower companies to measure and manage their carbon emissions. Terrascope and Thoughtworks partnered to build a decarbonization SaaS platform that allows businesses to rapidly and reliably assess their direct and indirect emissions (Scope 1, 2 and 3 emissions) and build a credible pathway to Net Zero.\"), Document(page_content=\"Source: https://www.thoughtworks.com/radar/tools/terragrunt\\n  Title: Terragrunt | Technology Radar\\n  Document language: \\n  Worth pursuing. It is important to understand how to build up this capability. Enterprises should try this technology on a project that can handle the risk.\\n\\nWe've used Terraform extensively to create and manage cloud infrastructure. In our experience with larger setups, where code is divided into modules that are included in different ways, teams eventually hit a wall of unavoidable repetition caused by a lack of flexibility. We've addressed this by using Terragrunt, a thin wrapper for Terraform that implements the practices advocated by Yevgeniy Brikman’s Terraform: Up and Running. We've found Terragrunt helpful because it encourages versioned modules and reusability for different environments. Lifecycle hooks are another useful feature providing additional flexibility. In terms of packaging, Terragrunt has the same limitations as Terraform: there is no proper way to define packages or dependencies between packages. As a workaround, you can use modules and specify a version associated with a Git tag.\"), Document(page_content='Source: https://www.thoughtworks.com/es-es/radar/tools/terragrunt\\n  Title: Terragrunt | Technology Radar\\n  Document language: es\\n  Vale la pena intentarlo. Es importante entender cómo construir esta habilidad. Las empresas deberían implementar esta tecnología en un proyecto que pueda manejar el riesgo.\\n\\nHemos usado Terraform extensivamente para crear y manejar infraestructuras en la nube. En nuestra experiencia con instalaciones más grandes, donde el código se encuentra dividido en módulos y las configuraciones se incluyen de diferentes maneras, los equipos eventualmente se tropiezan con un muro inevitable de repetición ocasionado por la falta de flexibilidad. Hemos resuelto esto utilizando Terragrunt, una delgada capa para Terraform que implementa las prácticas propuestas por Yevgeniy Brikman en su libro Terraform: Up and Running. Hemos encontrado útil a Terragrunt porque fomenta la creación de módulos versionados y la reutilización para distintos entornos. Los hooks para el ciclo de vida son otra funcionalidad útil que entrega flexibilidad adicional. En términos de empaquetado, Terragrunt tiene las mismas limitaciones que Terraform: no hay una forma apropiada para definir paquetes o dependencias entre paquetes. Como alternativa, puedes utilizar módulos y versiones específicas asociadas a una etiqueta de Git.'), Document(page_content='Source: https://www.thoughtworks.com/es-ec/radar/tools/terragrunt\\n  Title: Terragrunt | Technology Radar\\n  Document language: es\\n  Vale la pena intentarlo. Es importante entender cómo construir esta habilidad. Las empresas deberían implementar esta tecnología en un proyecto que pueda manejar el riesgo.\\n\\nHemos usado Terraform extensivamente para crear y manejar infraestructuras en la nube. En nuestra experiencia con instalaciones más grandes, donde el código se encuentra dividido en módulos y las configuraciones se incluyen de diferentes maneras, los equipos eventualmente se tropiezan con un muro inevitable de repetición ocasionado por la falta de flexibilidad. Hemos resuelto esto utilizando Terragrunt, una delgada capa para Terraform que implementa las prácticas propuestas por Yevgeniy Brikman en su libro Terraform: Up and Running. Hemos encontrado útil a Terragrunt porque fomenta la creación de módulos versionados y la reutilización para distintos entornos. Los hooks para el ciclo de vida son otra funcionalidad útil que entrega flexibilidad adicional. En términos de empaquetado, Terragrunt tiene las mismas limitaciones que Terraform: no hay una forma apropiada para definir paquetes o dependencias entre paquetes. Como alternativa, puedes utilizar módulos y versiones específicas asociadas a una etiqueta de Git.')]\n",
            "\n",
            "### QUESTION:\n",
            "What Thoughworks thinks Terragrunt is valuable for? [/INST]\n",
            "  According to the provided documents from Thoughtworks, Terragrunt is valued for its ability to encourage versioned modules and reusability for different environments, as well as its implementation of lifecycle hooks which provide additional flexibility. These features help address the issue of unavoidable repetition and lack of flexibility that can arise when managing larger setups with Terraform. However, it is important to note that Terragrunt shares the same limitations as Terraform regarding defining packages or dependencies between packages, and a workaround suggested is to use modules and specify a version associated with a Git tag.\n",
            "\n",
            "Reference(s):\n",
            "- <https://www.thoughtworks.com/radar/tools/terragrunt>\n",
            "- <https://www.thoughtworks.com/es-es/radar/tools/terragrunt>\n",
            "- <https://www.thoughtworks.com/es-ec/radar/tools/terragrunt>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"What is the recommended cloud platform to use?\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZv1onJ_V7eO",
        "outputId": "99b6081c-1144-4d71-9725-db963bcf4d9b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content=\"And the end state is also dependent on the specific context that you have and the architectural needs that you'll have for those systems at the end of it.\\n\\nNow, Alexandre, you did mention different vendors and different features and also aggressive commercial strategies from them. We've been talking in Thoughtworks about clouds' stickiness for awhile. So it's lock in to a specific cloud vendor. How do you see that? Are there tools that help us avoid that? Or to what extent do you really need to embrace whatever the cloud provider offers you?\\n\\nMm-hmm (affirmative). Yeah. That's a good point. Nowadays, we have a very varying level of infrastructure available in the cloud. Well, I think it's common sense from the main vendors that you can choose things that are closer to the bare metal, like creating your own virtual machines, dealing with network, having more control over the infrastructure. In the other side you have things like serverless, so you have functions or lambdas. So you put in that piece of code and you don't know where it's running, how is running, if it's just a server that is appearing and being spawned for you. So you have no control over the thing. So usually the left side of the spectrum is cheaper, so you have more control, and the right side is more expensive.\"), Document(page_content='in the cloud.’ They want to help their customers, and they want to run a profitable business. Cloud is just the means for them to do that.”'), Document(page_content=\"It's going to be in existence for a few months and then it might be discarded, something like that. There's no point in looking for a multi-cloud solution for that application. You can go for broke, use all of the vendor's capabilities. They give you a tremendous amount of labor saving innovations, and the cloud vendors are constantly rolling out new innovations that make it easier and easier for developers to use. On the other hand, if you have a really critical asset, something like a core system that actually runs your business, that's something that you probably want to manage the risks very carefully. And that's also an asset that's going to be much more long lived, that's an asset that you're going to have to maintain 5 or 10 years. So it's worth building in a certain level of portability there. It's worth building in a lot of abstraction and insulation from the cloud vendors proprietary services so that you can very quickly and cheaply port that later on.\"), Document(page_content=\"So the real key to Being able to balance the different cloud platforms is just simply your teams, or people having the capabilities to build, and understand what they're building. So in other words, I think there's a certain layer of knowledge that you have where, if you understand one cloud, you're able to apply that to other clouds. And the implementation details can vary across the different vendors, but that's the kind of thing that, if you build the right competence in your organization, you should be able to handle that.\\n\\nAnd I think one of the key recommendations I have on that, to people, is always to make sure you are building your own people's competence. So be very wary of bringing in a vendor. And as somebody who works at a vendor, in a consultancy, this is ... Something I recommend to my own clients is, don't bring us in to build and install something for you and then go away. Bring us in, or bring somebody in who is going to help your people build it so that they understand what they're building, and how to carry on building, and changing and adapting it as they go along. And that's your best defense against any kind of vendor lock in, is having your own people understand the stuff that they're building and using.\")]\n",
            "\n",
            "### QUESTION:\n",
            "What is the recommended cloud platform to use? [/INST]\n",
            "  The documents do not provide a definitive recommendation on which cloud platform to use. Instead, they suggest considering the specific context and architectural needs of your systems when making a decision. Factors such as cost, control, and the length of time you plan to use the application are discussed. The documents also emphasize the importance of building competence within your team to effectively manage and adapt to different cloud platforms.\n",
            "\n",
            "Source: All provided documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Native Model version\n",
        "quest = \"What is the recommended cloud platform to use?\"\n",
        "\n",
        "\n",
        "result = llm_chain.invoke(dict(question=quest, context=''))\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BhIY5owWVlE",
        "outputId": "c620da11-5fa8-4af3-a6fc-4ee940ae3ae7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide references links if available. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "\n",
            "\n",
            "### QUESTION:\n",
            "What is the recommended cloud platform to use? [/INST]\n",
            "  Based on publicly available information from Thoughtworks, they do not explicitly recommend a specific cloud platform over others. Thoughtworks is known for its technology-agnostic approach and provides services across various cloud platforms including Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), and IBM Cloud, among others. They help clients choose the right cloud platform based on their unique business needs and requirements. For more details, you can refer to their Cloud Services page at https://www.thoughtworks.com/services/cloud or contact them directly for personalized advice.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Native Model version\n",
        "quest = \"Recommend some graphic tool to explore code?\"\n",
        "\n",
        "\n",
        "result = llm_chain.invoke(dict(question=quest, context=''))\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaAMAnXzZLpH",
        "outputId": "2923bc6e-53d9-4181-cb15-88aa9827ed78"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "\n",
            "\n",
            "### QUESTION:\n",
            "Recommend some graphic tool to explore code? [/INST]\n",
            "  At Thoughtworks, we use various tools for exploring and visualizing code, but none of them are exclusively graphic or dedicated to this purpose. Here are a few popular tools among our developers that can be used for exploring code with some level of graphical representation:\n",
            "\n",
            "1. **GitHub**: GitHub provides several ways to visualize code through its web interface. You can view files side-by-side using file comparisons, examine commit history through graphs, and even create pull requests to propose changes. (Source: https://github.com/)\n",
            "\n",
            "2. **Visual Studio Code**: Visual Studio Code offers built-in features like the integrated terminal, debugger, and explorer windows that can help you navigate and understand code. Additionally, extensions such as \"CodeLens,\" \"Bracket Pair Colorizer,\" and \"Prettier\" can enhance the visual experience. (Source: https://code.visualstudio.com/)\n",
            "\n",
            "3. **Graphviz**: While not directly related to code exploration, Graphviz is a widely-used open-source graph visualization software. It's often employed to represent complex relationships between components in software systems, making it an essential tool for understanding larger codebases. (Source: https://graphviz.org/)\n",
            "\n",
            "4. **Docker**: Docker allows you to containerize applications and their dependencies, which can simplify the process of setting up development environments and exploring different versions of code. (Source: https://www.docker.com/)\n",
            "\n",
            "5. **Jupyter Notebook**: Jupyter Notebook is particularly useful for data scientists and researchers who work with Python, R, or Julia. It enables you to create and share documents containing live code, equations, visualizations, and narrative text. (Source: https://jupyter.org/)\n",
            "\n",
            "These tools should provide a good starting point for exploring code visually. However, it's important to note that each tool has its unique strengths and limitations, so you may need to experiment with a few options to find the best fit for your specific needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"Recommend some graphic tool to explore code?\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgM_KtaKX4tC",
        "outputId": "9dd186d1-bb0e-4ce0-d744-247be18d39e7"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content=\"So, a bit like Korny, I also settled on saying, let's use some basic building blocks if you will. And we assembled them with different toolings and the idea really was that you try to find something that can... If you talk about static analysis of code, I think we will probably go beyond that later in the conversation. But especially when you look at static analysis, to have one class of tools that provides analysis of the code base and outputs of textural format. And that is something you don't rewrite all the time. You have another class of tools that take some textual format and generates graphics and diagrams from it. And that again is something that is reusable.\\n\\nAnd that's not only for code analysis. You can use tools like D3 for example, which I've come to love over the last five years to do this visualization. So you can use on both ends of the process, existing tools, and the only thing you really have to write is the bit in the middle that transforms from one textual format, the output of the analysis step, into the input of the visualization step.\\n\\nAnd that have been a tool chain that at least for me has worked quite well, but it is incredibly hard to create a repeatable tool that you can just simply hand to somebody else to take this. It's really very custom made, and you basically have to teach people the basis of how these building blocks work together. And then again, it becomes a lot of effort to teach that.\"), Document(page_content=\"Any file format. Not any, because some languages don't really use any indentation at all, but in a lot of cases, it's a good way to quickly spot complexity, even in a non-programming language.\\n\\nThat's a very good point. In programming languages that have curly braces, a very simple visualization is simply to strip everything but the curly braces, and then start each line with the curly brace and you continue until you find the closing one. And then you go to the next line, which almost gives you if you look sideways, a column chart of the complexity of the methods and so on. So there's a lot of things you can do very simply again that help you. And as I said, I think we can apply these techniques.\\n\\nAnd one thing that I think in the infrastructure space that you mentioned, Ashok, we are seeing more tools than us again. So we have OpenTracing, we have all these tracing tools and there's a couple of commercial ones, I don't want to mention one and not the others, but we know who we're talking about here. We now have a space again where we have commercial interest and tooling. So again, maybe if we apply the same patterns and use the tools to extract some data, and then have some transformation we write ourselves and use some of the visualizations. And I know Korny has also used some pretty standard tools for the beautiful visualizations.\"), Document(page_content='Source: https://github.com/chanhx/crabviz\\n  Title: chanhx/crabviz: 🦀 A LSP-based interative call graph generator\\n  Document language: \\n  Crabviz is a LSP-based call graph generator. It leverages the Language Server Protocol to generate interactive call graphs, helps you visually explore source code.\\n• You can generate a call graph for selected files to get an overview, or a selected function to track the call hierarchy.\\n\\nSince Crabviz utilizes the capabilities of language servers, it is better suited as an IDE/editor extension than a standalone command line tool.\\n\\nIt is currently available on VS Code, and PRs for other editors are welcome.\\n\\nCrabviz is inspired by graphql-voyager and go-callvis.'), Document(page_content='But also, I think the legacy code thing is very big, the large systems. Legacy is probably unfair large systems of code I\\'ve had to deal with. I\\'ve gone into clients and they\\'ve said, \"Oh, we\\'ve got the system in the corner. This team of people have been working on it for 20 years. We don\\'t really know what\\'s there.\" And getting a quick grasp of what that code is about, that\\'s just essential and it\\'s almost impossible to do. You can load up an editor, but it doesn\\'t really give you a picture of what\\'s really there.\\n\\nEven just how big things are and how old they are, that sort of data, you want a quick 30,000 foot view. So, when I started finding tools that could do this, I got very keen on playing with them, and eventually I started writing my own tools because I got quite frustrated as well.\\n\\nYou mentioned frustration. What got you into actually trying to write your own tool?\\n\\nA couple of problems. One is that there\\'s a very high... A lot of these tools degrade over time. A lot of them were great when the author built them, somebody gets really keen, they build a tool, they make it work and they do everything. But a lot of them seem to just get bit rot. You go and pull them up and that\\'s five years old, and nobody\\'s changed them, interest has gone somewhere else and you can\\'t get it to run. A lot of my frustration was I went to look at some of the tools that were on Erik\\'s blog and I couldn\\'t get them to run.')]\n",
            "\n",
            "### QUESTION:\n",
            "Recommend some graphic tool to explore code? [/INST]\n",
            "  Based on the context provided, the speaker mentions using tools like D3.js for visualizing code analysis results. They also mention Crabviz, which is a LSP-based call graph generator that generates interactive call graphs and helps users visually explore source code (source: <https://github.com/chanhx/crabviz>). These tools can be useful for exploring code and gaining insights into its structure and complexity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"\"\"\n",
        "What is the recommend approach to integrate LLMs ?:\n",
        "- Use private LLMs like Chatgpt, Llama\n",
        "- Use finetuned open-source models\n",
        "- Use Retrieval-Augmented Generation (RAG) applications\n",
        "\"\"\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW7y9aJEZ-vE",
        "outputId": "b20f06d8-037e-4721-f42c-190c11a5cd45"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content=\"Neal: I think there's an interesting split between the general use or the very general purpose and very surprisingly useful, but also hyper-specific context and training an LLM around a very specific context, either because language is hyper-precise like in a legal or medical realm or in gaming or something like that where you want a narrow way to look at that. That's one interesting aspect. The other interesting aspect is, as technologists, we love to peel back abstraction layers and see how things work. That's, you get into LLMs and then start looking at the individual pieces and how they fit together, which is always fascinating.\\n\\nRebecca: One of the interesting use cases that I've heard about, and we actually have, internally, a system that will allow you to do this, is you can basically upload a specific document, maybe it's a manual or a tutorial or something, and then you can use the chat interface to query that document. Rather than having to page through or page through on the screen, you simply ask, what is the interface for this function? It will return it to you.\\n\\nThat's one of those very specific contexts where you're using the foundational model as your effectively interface mechanism, but the content itself is very specific to, and this is the document I want you to retrieve from. These are the things I want to know about. From a knowledge management perspective, that can be incredibly powerful.\"), Document(page_content='Source: https://www.thoughtworks.com/en-cl/radar/tools/llama-2\\n  Title: Llama 2 | Technology Radar\\n  Document language: en\\n  Worth exploring with the goal of understanding how it will affect your enterprise.\\n\\nLlama 2, from Meta, is a powerful language model that is free for both research and commercial use. It\\'s available both as a raw pretrained model and, fine-tuned, as Llama-2-chat for conversation and Code Llama for code completion. Since it\\'s available in a variety of sizes — 7B, 13B, and 70B — Llama 2 is a good option for a self-hosted LLM, if you want to control your data.\\n\\nMeta describes Llama 2 as \"open source,\" a claim that has attracted some criticism. Meta\\'s license and acceptable use policy put restrictions on commercial use for some users and also restricts the use of the model and software for certain purposes. The Llama 2 training data is not open, which can hamper the ability to understand and change the model. That said, the availability of a powerful, capable model in at least a \"semi-open\" form is welcome.'), Document(page_content='Source: https://www.thoughtworks.com/en-br/radar/tools/llama-2\\n  Title: Llama 2 | Technology Radar\\n  Document language: en\\n  Worth exploring with the goal of understanding how it will affect your enterprise.\\n\\nLlama 2, from Meta, is a powerful language model that is free for both research and commercial use. It\\'s available both as a raw pretrained model and, fine-tuned, as Llama-2-chat for conversation and Code Llama for code completion. Since it\\'s available in a variety of sizes — 7B, 13B, and 70B — Llama 2 is a good option for a self-hosted LLM, if you want to control your data.\\n\\nMeta describes Llama 2 as \"open source,\" a claim that has attracted some criticism. Meta\\'s license and acceptable use policy put restrictions on commercial use for some users and also restricts the use of the model and software for certain purposes. The Llama 2 training data is not open, which can hamper the ability to understand and change the model. That said, the availability of a powerful, capable model in at least a \"semi-open\" form is welcome.'), Document(page_content='Source: https://www.thoughtworks.com/en-au/radar/tools/llama-2\\n  Title: Llama 2 | Technology Radar\\n  Document language: en\\n  Worth exploring with the goal of understanding how it will affect your enterprise.\\n\\nLlama 2, from Meta, is a powerful language model that is free for both research and commercial use. It\\'s available both as a raw pretrained model and, fine-tuned, as Llama-2-chat for conversation and Code Llama for code completion. Since it\\'s available in a variety of sizes — 7B, 13B, and 70B — Llama 2 is a good option for a self-hosted LLM, if you want to control your data.\\n\\nMeta describes Llama 2 as \"open source,\" a claim that has attracted some criticism. Meta\\'s license and acceptable use policy put restrictions on commercial use for some users and also restricts the use of the model and software for certain purposes. The Llama 2 training data is not open, which can hamper the ability to understand and change the model. That said, the availability of a powerful, capable model in at least a \"semi-open\" form is welcome.')]\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What is the recommend approach to integrate LLMs ?:\n",
            "- Use private LLMs like Chatgpt, Llama\n",
            "- Use finetuned open-source models\n",
            "- Use Retrieval-Augmented Generation (RAG) applications\n",
            " [/INST]\n",
            "  Based on the provided context, there is no clear answer regarding the recommended approach to integrate LLMs in the text. However, the documents mention the use of Llama 2, which is available both as a raw pretrained model and fine-tuned versions like Llama-2-chat and Code Llama. This suggests that using finetuned open-source models could be an option. Additionally, the context discusses using LLMs in specific contexts by querying documents, which might involve Retrieval-Augmented Generation (RAG) applications. Therefore, a combination of using finetuned open-source models, RAG applications, and possibly private LLMs like Chatgpt or Llama could be considered for integration depending on the specific requirements and constraints.\n",
            "\n",
            "Sources:\n",
            "- Document(page_content=\"...\")\n",
            "- <https://www.thoughtworks.com/en-cl/radar/tools/llama-2>\n",
            "- <https://www.thoughtworks.com/en-br/radar/tools/llama-2>\n",
            "- <https://www.thoughtworks.com/en-au/radar/tools/llama-2>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Native Model version\n",
        "quest = \"\"\"\n",
        "What is the recommend approach to integrate LLMs ?:\n",
        "- Use private LLMs like Chatgpt, Llama\n",
        "- Use finetuned open-source models\n",
        "- Use Retrieval-Augmented Generation (RAG) applications\n",
        "\"\"\"\n",
        "\n",
        "result = llm_chain.invoke(dict(question=quest, context=''))\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49tIUFLqa4Yb",
        "outputId": "352b2725-e064-478c-95e5-53ca8182610f"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What is the recommend approach to integrate LLMs ?:\n",
            "- Use private LLMs like Chatgpt, Llama\n",
            "- Use finetuned open-source models\n",
            "- Use Retrieval-Augmented Generation (RAG) applications\n",
            " [/INST]\n",
            "  Based on my knowledge from publicly available information about Thoughtworks and their approach to Large Language Models (LLMs), I would suggest using a combination of finetuned open-source models and Retrieval-Augmented Generation (RAG) applications for integration.\n",
            "\n",
            "Thoughtworks has been actively exploring the use of large language models in various projects and solutions. They have published several articles and blog posts discussing their experiences with fine-tuning open-source models such as Hugging Face Transformers and T5. These models can be integrated into applications by fine-tuning them on specific datasets or tasks.\n",
            "\n",
            "Regarding the use of private LLMs like ChatGPT or Llama, while Thoughtworks may explore these models for internal research and development purposes, there isn't enough publicly available information to determine if they are recommended for general use in client projects or if they are officially supported by Thoughtworks.\n",
            "\n",
            "As for RAG applications, Thoughtworks has shown interest in this area as well. In a blog post titled \"Retrieval-augmented generation: A new paradigm for conversational AI,\" they discuss how RAG can improve the performance and efficiency of conversational AI systems. By integrating retrieval models with LLMs, RAG applications can provide more accurate and relevant responses by combining the strengths of both models.\n",
            "\n",
            "Sources:\n",
            "1. Thoughtworks Blog: https://www.thoughtworks.com/insights/blog/retrieval-augmented-generation-new-paradigm-conversational-ai\n",
            "2. Thoughtworks Blog: https://www.thoughtworks.com/insights/blog/fine-tuning-large-language-models-with-hugging-face-transformers\n",
            "3. Thoughtworks Tech Radar: https://techradar.thoughtworks.com/technologies/natural-language-processing.html#when-to-use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"\"\"\n",
        "What about use Retrieval-Augmented Generation (RAG) applications, is that something recommended?, mention some tools for it.\n",
        "\"\"\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkJDvH4wcACF",
        "outputId": "2f5c521a-2d35-4ff1-a690-eba43ce1aa6b"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content='Source: https://www.thoughtworks.com/de-de/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: \\n  Retrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.'), Document(page_content='Source: https://github.com/vanna-ai/vanna\\n  Title: vanna-ai/vanna: 🤖 Chat with your SQL database 📊. Accurate Text-to-SQL Generation via LLMs using RAG 🔄.\\n  Document language: \\n  Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality.\\n\\nVanna works in two easy steps - train a RAG \"model\" on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database.\\n\\nIf you don\\'t know what RAG is, don\\'t worry -- you don\\'t need to know how this works under the hood to use it. You just need to know that you \"train\" a model, which stores some metadata and then use it to \"ask\" questions.\\n\\nSee the base class for more details on how this works under the hood.\\n\\nThese are some of the user interfaces that we\\'ve built using Vanna. You can use these as-is or as a starting point for your own custom interface.\\n\\nSee the documentation for specifics on your desired database, LLM, etc.\\n\\nIf you want to get a feel for how it works after training, you can try this Colab notebook.\\n\\nThere are a number of optional packages that can be installed so see the documentation for more details.\\n\\nSee the documentation if you\\'re customizing the LLM or vector database.\\n\\nYou may or may not need to run these commands depending on your use case. See the documentation for more details.\\n\\nThese statements are shown to give you a feel for how it works.'), Document(page_content='Source: https://www.thoughtworks.com/en-br/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: en\\n  We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects.\\n\\nRetrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.'), Document(page_content='Source: https://www.thoughtworks.com/en-cn/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: en\\n  We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects.\\n\\nRetrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.')]\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What about use Retrieval-Augmented Generation (RAG) applications, is that something recommended?, mention some tools for it.\n",
            " [/INST]\n",
            "  Based on the provided context from Thoughtworks' Technology Radar, yes, Retrieval-Augmented Generation (RAG) is recommended for improving the quality of responses generated by large language models (LLMs). RAG involves storing relevant and trustworthy documents in databases that support vector data types or efficient document search, such as pgvector, Qdrant, or Elasticsearch Relevance Engine. These documents are then retrieved and combined with the prompt to provide richer context to the LLM, resulting in higher quality output and reduced hallucinations.\n",
            "\n",
            "One open-source Python framework for implementing RAG is Vanna, which is mentioned in the second document provided. Vanna is an MIT-licensed tool for SQL generation and related functionality using RAG. It allows users to train a model on their data and then ask questions that will return SQL queries that can be set up to automatically run on their database.\n",
            "\n",
            "Another tool mentioned in the context is the Jugalbandi AI Platform, which has also successfully implemented RAG in several projects. However, the exact implementation details or availability of this platform are not provided in the context.\n",
            "\n",
            "Overall, RAG is recommended for improving the quality of responses generated by LLMs, and tools like Vanna can be used to implement this approach.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Native Model version\n",
        "quest = \"\"\"\n",
        "What about use Retrieval-Augmented Generation (RAG) applications, is that something recommended?, mention some tools for it.\n",
        "\"\"\"\n",
        "\n",
        "result = llm_chain.invoke(dict(question=quest, context=''))\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovttXEFwcllx",
        "outputId": "45c6c68c-c5fe-4137-a2a5-ed896129267c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What about use Retrieval-Augmented Generation (RAG) applications, is that something recommended?, mention some tools for it.\n",
            " [/INST]\n",
            "  According to Thoughtworks' technology radar reports, Retrieval-Augmented Generation (RAG) applications have been identified as a \"radar\" technology since Q3 2021. RAG applications combine retrieval systems and generation models to provide more accurate and contextually relevant responses than traditional retrieval systems alone. This approach can be particularly useful in areas like customer service, content generation, and information retrieval.\n",
            "\n",
            "Some popular tools for building RAG applications include:\n",
            "\n",
            "1. DALL-E 2: A text-to-image model from OpenAI that can generate images based on descriptions.\n",
            "2. BERT (Bidirectional Encoder Representations from Transformers): A natural language processing model developed by Google that can understand the context of words in a sentence.\n",
            "3. Hugging Face Transformers: An open-source library for state-of-the-art NLP models, including BERT and other transformer models.\n",
            "4. TensorFlow: A popular machine learning framework from Google for building and training machine learning models, including RAG applications.\n",
            "5. Elasticsearch: A search engine that can be used as a retrieval system for RAG applications.\n",
            "\n",
            "For more detailed information on these technologies and their relevance to RAG applications, you may want to refer to Thoughtworks' Technology Radar reports, specifically the ones from Q3 2021 and later. Here are the links to those reports:\n",
            "\n",
            "* Q3 2021: https://www.thoughtworks.com/insights/technology-radar/q3-2021\n",
            "* Q4 2021: https://www.thoughtworks.com/insights/technology-radar/q4-2021\n",
            "* Q1 2022: https://www.thoughtworks.com/insights/technology-radar/q1-2022\n",
            "\n",
            "These reports provide insights into the current trends and recommendations in technology, including RAG applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Native Model version\n",
        "quest = \"\"\"\n",
        "What are the tools to implement Retrieval-Augmented Generation (RAG) applications?.\n",
        "\"\"\"\n",
        "\n",
        "result = llm_chain.invoke(dict(question=quest, context=''))\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_3blFYTd1gE",
        "outputId": "771fe163-4914-404d-e383-98ebfc8f1ad9"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What are the tools to implement Retrieval-Augmented Generation (RAG) applications?.\n",
            " [/INST]\n",
            "  Based on my knowledge from publicly available information about Thoughtworks and their work in the field of Retrieval-Augmented Generation (RAG), I cannot find a definitive list of specific tools that Thoughtworks recommends for implementing RAG applications. However, they have published several articles and blog posts discussing the concept and implementation of RAG.\n",
            "\n",
            "One such article titled \"Retrieval-augmented generation: Combining human creativity with AI\" published on Thoughtworks' Insights blog discusses the importance of RAG and how it can be implemented using various techniques and technologies like deep learning models, retrieval systems, and natural language processing (NLP). The article also mentions some popular NLP libraries and frameworks like spaCy, NLTK, and Hugging Face Transformers for building RAG systems.\n",
            "\n",
            "Another blog post titled \"Retrieval-augmented generation: A new paradigm for conversational AI\" published on Thoughtworks' website discusses the potential of RAG in conversational AI and provides an overview of the architecture and components required to build a RAG system. It mentions the use of databases, search engines, and machine learning models for retrieving relevant information and generating responses.\n",
            "\n",
            "Therefore, while there isn't a definitive list of tools provided by Thoughtworks for implementing RAG applications, the articles suggest using a combination of deep learning models, NLP libraries, databases, search engines, and machine learning models.\n",
            "\n",
            "Sources:\n",
            "1. Retrieval-augmented generation: Combining human creativity with AI - https://www.thoughtworks.com/insights/blog/retrieval-augmented-generation-combining-human-creativity-with-ai\n",
            "2. Retrieval-augmented generation: A new paradigm for conversational AI - https://www.thoughtworks.com/en-us/services/artificial-intelligence/conversational-ai/retrieval-augmented-generation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quest = \"\"\"\n",
        "What are the tools to implement Retrieval-Augmented Generation (RAG) applications?.\n",
        "\"\"\"\n",
        "rag_chain = (\n",
        " {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke(quest)\n",
        "print(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aD3x9rrveBTd",
        "outputId": "82be6d76-f299-4ae7-9acd-911d26034d1d"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "### [INST] Instruction: Answer the question based on your knowledge in public documents of Thoughtworks company. Provide source links if there is any in the context. If there is no clear answer just write 'NOT FOUND'.\n",
            "\n",
            "Here is context to help:\n",
            "[Document(page_content='Source: https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: \\n  We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects.\\n\\nRetrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.'), Document(page_content='Source: https://www.thoughtworks.com/de-de/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: \\n  Retrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.'), Document(page_content='Source: https://www.thoughtworks.com/en-br/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: en\\n  We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects.\\n\\nRetrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.'), Document(page_content='Source: https://www.thoughtworks.com/en-sg/radar/techniques/retrieval-augmented-generation-rag\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language: en\\n  We feel strongly that the industry should be adopting these items. We use them when appropriate on our projects.\\n\\nRetrieval-augmented generation (RAG) is the preferred pattern for our teams to improve the quality of responses generated by a large language model (LLM). We’ve successfully used it in several projects, including the popular Jugalbandi AI Platform. With RAG, information about relevant and trustworthy documents — in formats like HTML and PDF — are stored in databases that supports a vector data type or efficient document search, such as pgvector, Qdrant or Elasticsearch Relevance Engine. For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial. We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks. This is often a difficult problem, and one approach is to have the chunks overlap to a certain extent.')]\n",
            "\n",
            "### QUESTION:\n",
            "\n",
            "What are the tools to implement Retrieval-Augmented Generation (RAG) applications?.\n",
            " [/INST]\n",
            "  According to the provided context from Thoughtworks' Technology Radar, the following tools are mentioned as suitable for implementing Retrieval-Augmented Generation (RAG) applications:\n",
            "\n",
            "1. pgvector: A vector database that supports PostgreSQL.\n",
            "2. Qdrant: An open-source vector database.\n",
            "3. Elasticsearch Relevance Engine: A search engine that uses machine learning to improve the relevancy of search results.\n",
            "\n",
            "These tools can be used to store and efficiently search information about relevant and trustworthy documents for RAG applications. They support vector data types or efficient document search, making them suitable for this purpose.\n",
            "\n",
            "Reference:\n",
            "[Document(page_content='Source: <https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag>\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language:\\n ...\\n  For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial.\\n  We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks...', line=5)]\n",
            "\n",
            "[Document(page_content='Source: <https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag>\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language:\\n ...\\n  For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial.\\n  We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks...', line=14)]\n",
            "\n",
            "[Document(page_content='Source: <https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag>\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language:\\n ...\\n  For a given prompt, the database is queried to retrieve relevant documents, which are then combined with the prompt to provide richer context to the LLM. This results in higher quality output and greatly reduced hallucinations. The context window — which determines the maximum size of the LLM input — is limited, which means that selecting the most relevant documents is crucial.\\n  We improve the relevancy of the content that is added to the prompt by reranking. Similarly, the documents are usually too large to calculate an embedding, which means they must be split into smaller chunks...', line=20)]\n",
            "\n",
            "[Document(page_content='Source: <https://www.thoughtworks.com/radar/techniques/retrieval-augmented-generation-rag>\\n  Title: Retrieval-augmented generation (RAG) | Technology Radar\\n  Document language:\\n ...\\\n"
          ]
        }
      ]
    }
  ]
}